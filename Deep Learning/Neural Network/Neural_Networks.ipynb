{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks"
      ],
      "metadata": {
        "id": "DrP0jYxEfFBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) What is Deep Learning? Briefly describe how it evolved and how it differs from traditional machine learning.\n",
        "\n",
        "->\n",
        "\n",
        "Deep Learning is a subset of Machine Learning that uses **artificial neural networks** with many layers (hence “deep”) to automatically learn complex patterns from data.  \n",
        "It evolved from traditional ML by overcoming the limitation of manual feature extraction — instead, deep networks learn hierarchical features directly from raw data.\n",
        "\n",
        "Traditional ML models like Decision Trees or SVMs rely heavily on human-engineered features, while Deep Learning leverages **representation learning**, where hidden layers capture increasingly abstract patterns (e.g., edges → shapes → objects in images).\n",
        "\n",
        "**Evolution factors:**\n",
        "- Availability of large datasets  \n",
        "- Powerful GPUs and TPUs  \n",
        "- Advanced algorithms (e.g., backpropagation, dropout)\n",
        "\n",
        "Deep Learning has revolutionized fields like computer vision, speech recognition, and NLP.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2) Explain the basic architecture and functioning of a Perceptron. What are its limitations?\n",
        "\n",
        "->\n",
        "\n",
        "A **Perceptron** is the simplest form of a neural network, consisting of:\n",
        "- Inputs (features)\n",
        "- Weights and bias\n",
        "- Activation function\n",
        "- Output node\n",
        "\n",
        "**Computation:**\n",
        "\n",
        "$$\n",
        "z = \\sum_{i=1}^{n} w_i x_i + b\n",
        "$$\n",
        "\n",
        "If z exceeds a threshold, it outputs 1; otherwise 0.\n",
        "\n",
        "**Limitations:**\n",
        "- Can only solve **linearly separable** problems (fails on XOR-type data)  \n",
        "- No hidden layers → limited learning capacity  \n",
        "- Sensitive to feature scaling\n",
        "\n",
        "Hence, Multi-Layer Perceptrons (MLPs) were developed to learn non-linear relationships.\n",
        "\n",
        "\n",
        "\n",
        "3) Describe the purpose of activation function in neural networks. Compare Sigmoid, ReLU, and Tanh functions.\n",
        "\n",
        "->\n",
        "\n",
        "Activation functions introduce **non-linearity** in neural networks so they can model complex data.\n",
        "\n",
        "**1. Sigmoid Function**\n",
        "\n",
        "$$\n",
        "f(x) = \\frac{1}{1 + e^{-x}}\n",
        "$$\n",
        "\n",
        "Range → (0, 1)  \n",
        "✅ Smooth gradient  \n",
        "❌ Causes vanishing gradients\n",
        "\n",
        "\n",
        "**2. Tanh Function**\n",
        "\n",
        "$$\n",
        "f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "$$\n",
        "\n",
        "Range → (-1, 1)  \n",
        "✅ Zero-centered  \n",
        "❌ Still suffers from vanishing gradients\n",
        "\n",
        "\n",
        "**3. ReLU (Rectified Linear Unit)**\n",
        "\n",
        "$$\n",
        "f(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "✅ Fast, efficient  \n",
        "✅ Helps avoid vanishing gradient  \n",
        "❌ “Dead neurons” for negative inputs\n",
        "\n",
        "➡ **ReLU** is most commonly used today.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4) What is the difference between Loss function and Cost function in neural networks? Provide examples.\n",
        "\n",
        "->\n",
        "\n",
        "Both measure model performance but differ in **scope**:\n",
        "\n",
        "- **Loss Function:** error for a single observation  \n",
        "- **Cost Function:** average of all individual losses\n",
        "\n",
        "**Mathematical form:**\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} L(y_i, \\hat{y}_i)\n",
        "$$\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "*Mean Squared Error (MSE)*  \n",
        "$$\n",
        "L(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2\n",
        "$$\n",
        "\n",
        "*Binary Cross-Entropy Loss*  \n",
        "$$\n",
        "L(y, \\hat{y}) = -[y \\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y})]\n",
        "$$\n",
        "\n",
        "**Summary:**  \n",
        "Loss → single sample | Cost → dataset average\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5) What is the role of optimizers in neural networks? Compare Gradient Descent, Adam, and RMSprop.\n",
        "\n",
        "->\n",
        "\n",
        "Optimizers control **how weights are updated** to minimize the loss function — determining convergence speed and stability.\n",
        "\n",
        "\n",
        "**1. Gradient Descent**\n",
        "\n",
        "$$\n",
        "w = w - \\eta \\frac{\\partial J}{\\partial w}\n",
        "$$\n",
        "\n",
        "✅ Simple and effective  \n",
        "❌ May get stuck in local minima  \n",
        "❌ Sensitive to learning rate\n",
        "\n",
        "\n",
        "**2. RMSprop**\n",
        "\n",
        "Uses moving average of squared gradients to adapt learning rates.  \n",
        "✅ Good for non-stationary problems  \n",
        "✅ Faster convergence than plain GD\n",
        "\n",
        "---\n",
        "\n",
        "**3. Adam (Adaptive Moment Estimation)**\n",
        "\n",
        "Combines momentum + RMSprop:\n",
        "\n",
        "$$\n",
        "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
        "$$\n",
        "\n",
        "$$\n",
        "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "w = w - \\eta \\frac{m_t}{\\sqrt{v_t} + \\epsilon}\n",
        "$$\n",
        "\n",
        "✅ Fast, adaptive, robust  \n",
        "✅ Default optimizer for most networks"
      ],
      "metadata": {
        "id": "P0zUsiHvm7UG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use NumPy, Matplotlib, and Tensorflow/Keras for implementation."
      ],
      "metadata": {
        "id": "XvXMN0_slgFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "6) Write a Python program to implement a single-layer perceptron from scratch using NumPy\n",
        "to solve the logical AND gate.\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "# Training data for AND gate\n",
        "# Inputs (with bias term appended as 1)\n",
        "X = np.array([[0, 0, 1],\n",
        "              [0, 1, 1],\n",
        "              [1, 0, 1],\n",
        "              [1, 1, 1]], dtype=float)  # last column is bias input (x0 = 1)\n",
        "\n",
        "# Targets\n",
        "y = np.array([0, 0, 0, 1], dtype=float)\n",
        "\n",
        "# Perceptron parameters\n",
        "np.random.seed(42)\n",
        "weights = np.random.uniform(-1, 1, size=(3,))  # two feature weights + bias weight\n",
        "lr = 0.1\n",
        "n_epochs = 50\n",
        "\n",
        "def step(x):\n",
        "    return 1.0 if x >= 0 else 0.0\n",
        "\n",
        "# Training loop (Perceptron learning rule)\n",
        "for epoch in range(n_epochs):\n",
        "    errors = 0\n",
        "    for xi, target in zip(X, y):\n",
        "        activation = np.dot(weights, xi)\n",
        "        pred = step(activation)\n",
        "        update = lr * (target - pred)\n",
        "        if update != 0:\n",
        "            weights += update * xi\n",
        "            errors += 1\n",
        "    # Early stop if no errors\n",
        "    if errors == 0:\n",
        "        break\n",
        "\n",
        "# Results\n",
        "print(\"Trained weights (w1, w2, bias):\", weights)\n",
        "print(\"Predictions on AND inputs:\")\n",
        "for xi in X:\n",
        "    print(f\"Input: {xi[:2]} -> Pred:\", step(np.dot(weights, xi)))"
      ],
      "metadata": {
        "id": "MJrAqKpqmX0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "7) Implement and visualize Sigmoid, ReLU, and Tanh activation functions using Matplotlib.\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace(-6, 6, 500)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "y_sig = sigmoid(x)\n",
        "y_relu = relu(x)\n",
        "y_tanh = tanh(x)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(x, y_sig, label=\"Sigmoid\", linewidth=2)\n",
        "plt.plot(x, y_relu, label=\"ReLU\", linewidth=2)\n",
        "plt.plot(x, y_tanh, label=\"Tanh\", linewidth=2)\n",
        "plt.axvline(0, color='gray', linewidth=0.5)\n",
        "plt.axhline(0, color='gray', linewidth=0.5)\n",
        "plt.title(\"Activation Functions: Sigmoid, ReLU, Tanh\")\n",
        "plt.xlabel(\"Input\")\n",
        "plt.ylabel(\"Output\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vKYRM7camlV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "8) Use Keras to build and train a simple multilayer neural network on the MNIST digits dataset.\n",
        "Print the training accuracy.\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load MNIST\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess\n",
        "x_train = x_train.reshape(-1, 28 * 28).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1, 28 * 28).astype(\"float32\") / 255.0\n",
        "y_train_cat = to_categorical(y_train, 10)\n",
        "y_test_cat = to_categorical(y_test, 10)\n",
        "\n",
        "# Build model\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(28*28,)),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train (set epochs small to be notebook-friendly; increase as needed)\n",
        "history = model.fit(x_train, y_train_cat, epochs=5, batch_size=128, validation_split=0.1, verbose=2)\n",
        "\n",
        "# Print final training accuracy\n",
        "train_acc = history.history['accuracy'][-1]\n",
        "print(f\"Final training accuracy (last epoch): {train_acc:.4f}\")"
      ],
      "metadata": {
        "id": "6MbiezoHmsjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "9) Visualize the loss and accuracy curves for a neural network model trained on the Fashion MNIST dataset.\n",
        "Interpret the training behavior.\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess Fashion MNIST\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train = x_train[..., None]  # add channel dimension\n",
        "x_test = x_test[..., None]\n",
        "y_train_cat = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test_cat = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Simple CNN\n",
        "model_f = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_f.compile(optimizer='adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Train briefly (adjust epochs if you want more)\n",
        "history_f = model_f.fit(x_train, y_train_cat, epochs=6, batch_size=256, validation_split=0.1, verbose=2)\n",
        "\n",
        "# Plot loss and accuracy\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history_f.history['loss'], label='train loss')\n",
        "plt.plot(history_f.history['val_loss'], label='val loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history_f.history['accuracy'], label='train acc')\n",
        "plt.plot(history_f.history['val_accuracy'], label='val acc')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Interpretation (print summary)\n",
        "print(\"Interpretation tips:\")\n",
        "print(\"- If training loss decreases and training accuracy increases while validation loss decreases and val accuracy increases: model is learning and generalizing.\")\n",
        "print(\"- If training loss decreases but validation loss increases (val acc drops): model is overfitting.\")\n",
        "print(\"- If both train and val loss stay high: underfitting or model too simple / need more training.\")\n",
        "print(\"- Use early stopping, dropout, data augmentation or regularization to handle overfitting.\")"
      ],
      "metadata": {
        "id": "DfBP6sKkmzGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10) You are working on a project for a bank that wants to automatically detect fraudulent transactions. The dataset is large, imbalanced, and contains structured features like transaction amount, merchant ID, and customer location. The goal is to classify each transaction as fraudulent or legitimate.\n",
        "\n",
        "->\n",
        "\n",
        "**Real-time Fraud Detection — End-to-End Deep Learning Workflow**\n",
        "\n",
        "**1. Problem framing & data**\n",
        "- **Task:** Binary classification (fraud = 1, legit = 0).  \n",
        "- **Data sources:** transaction_amount, merchant_id, customer_id/location, timestamp, device info, historical customer behavior, engineered features (rolling averages, time-since-last-transaction, merchant risk score).  \n",
        "- **Challenges:** severe class imbalance, concept drift (fraud evolves), high volume (latency constraints), high-cardinality categorical features.\n",
        "\n",
        "**2. Model design**\n",
        "- Use a **multilayer neural network (MLP)** for tabular data with specialized handling for categorical IDs:\n",
        "  - Numeric inputs → standardize (e.g., `StandardScaler`).\n",
        "  - High-cardinality categorical features (merchant_id, customer_id) → integer encode → **embedding layers**.\n",
        "  - Concatenate embeddings + numeric features → dense stack.\n",
        "- Example architecture (conceptual):\n",
        "  - Inputs: numeric vector \\(x_{num}\\), embeddings \\(e_{merchant}, e_{customer}\\)  \n",
        "  - Concatenate: \\(h_0 = [x_{num}, e_{merchant}, e_{customer}]\\)  \n",
        "  - Dense(256, ReLU) → BatchNorm → Dropout(0.3)  \n",
        "  - Dense(128, ReLU) → BatchNorm → Dropout(0.3)  \n",
        "  - Dense(64, ReLU) → Dropout(0.2)  \n",
        "  - Output: Dense(1, activation='sigmoid') → probability of fraud\n",
        "\n",
        "**3. Activation & loss**\n",
        "- **Hidden layers:** ReLU (or variants like Leaky ReLU) for fast convergence and to mitigate vanishing gradients.  \n",
        "- **Output layer:** Sigmoid to produce probability \\(p=\\hat{y}\\in(0,1)\\).  \n",
        "- **Primary loss:** Binary Cross-Entropy (BCE):\n",
        "  $$\n",
        "  L_{\\text{BCE}}(y,p) = -\\big[y\\log(p) + (1-y)\\log(1-p)\\big]\n",
        "  $$\n",
        "- **For imbalance:**  \n",
        "  - **Weighted BCE:** multiply BCE for class 1 (fraud) by a larger weight \\(w_1\\).  \n",
        "  - **Focal Loss** (focus on hard examples):\n",
        "  $$\n",
        "  FL(p_t) = - (1 - p_t)^\\gamma \\log(p_t)\n",
        "  $$\n",
        "  where \\(p_t = p\\) if \\(y=1\\), else \\(p_t=1-p\\); \\(\\gamma>0\\) focuses training on hard-to-classify examples.\n",
        "\n",
        "**4. Handling class imbalance (training & evaluation)**\n",
        "- **Data-level methods:**\n",
        "  - Careful oversampling of minority (fraud) using time-aware approaches (avoid leakage). Consider SMOTE-like techniques for training but beware temporal dependencies.\n",
        "  - Undersampling the majority class for experiments (not always production-viable).\n",
        "- **Algorithm-level methods:**\n",
        "  - Use **class weights** during training (Keras `class_weight`) or use focal loss.\n",
        "  - Threshold tuning: choose decision threshold to meet business constraints (maximize recall at acceptable precision).\n",
        "- **Evaluation metrics:** use **Precision-Recall AUC (PR-AUC)**, **Recall (sensitivity)**, **Precision**, **F1**, and business-cost metrics (cost-weighted confusion matrix). ROC-AUC can be misleading under heavy imbalance — prefer PR-AUC.\n",
        "- **Validation strategy:** time-aware split (train on past, validate on later window) to avoid leakage; consider rolling-window cross-validation.\n",
        "\n",
        "**5. Optimizer & training details**\n",
        "- **Optimizer:** Adam (or AdamW) — adaptive, fast convergence. Use learning rate scheduling (ReduceLROnPlateau) and warm restarts if needed.  \n",
        "- **Adam moment updates (concept):**\n",
        "  $$\n",
        "  m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t,\\quad\n",
        "  v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\n",
        "  $$\n",
        "  $$\n",
        "  \\hat{m}_t = \\frac{m_t}{1-\\beta_1^t},\\quad\n",
        "  \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}\n",
        "  $$\n",
        "  $$\n",
        "  \\theta_t = \\theta_{t-1} - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
        "  $$\n",
        "\n",
        "**6. Preventing overfitting & improving robustness**\n",
        "- Regularization: Dropout, L2 weight decay (or AdamW), Batch Normalization.  \n",
        "- Feature engineering & noise-robust features (avoid overfitting to spurious correlations).  \n",
        "- Early stopping on validation PR-AUC.  \n",
        "- Ensembling: combine NN with tree-based models (e.g., XGBoost) — often improves performance on tabular data.  \n",
        "- Calibrate probabilities (Platt scaling / isotonic) if using model probabilities in downstream decision-making.\n",
        "\n",
        "**7. Real-time / production considerations**\n",
        "- **Feature store & online features:** compute and serve up-to-date rolling statistics in real-time (or near real-time).  \n",
        "- **Latency:** keep the model small enough to meet inference-time SLAs; use model quantization or distilled models if necessary.  \n",
        "- **Monitoring:** track data drift, concept drift, input distributions, prediction distributions, and business KPIs (false negative alerts). Automate alerting when performance degrades.  \n",
        "- **Retraining strategy:** scheduled retraining plus drift-triggered retraining; maintain model lineage and versioning.  \n",
        "- **Explainability & audit:** store features and predictions for every decision for auditing; use SHAP/LIME for root-cause analysis on flagged transactions.\n",
        "\n",
        "**8. Deployment & operations**\n",
        "- Serve via low-latency API (REST/gRPC) or integrate into streaming pipeline (Kafka → model microservice).  \n",
        "- Build fallback/business rules for critical low-confidence decisions.  \n",
        "- Log predictions and outcomes to support continuous learning and regulatory audit.\n",
        "\n",
        "**9. Business alignment**\n",
        "- Optimize for business cost: weight false negatives (missed fraud) and false positives (customer friction) according to business loss matrix.  \n",
        "- Provide metrics dashboards for analysts to review flagged transactions and adjust thresholds.\n",
        "\n",
        "**Summary (one-line):**  \n",
        "Use a multilayer NN with embeddings for categorical IDs, ReLU hidden activations, sigmoid output, weighted BCE or focal loss, Adam/AdamW optimizer, time-aware training and PR-AUC-based monitoring, plus robust production infra (feature store, drift detection, model versioning) to deliver accurate, low-latency fraud detection with ongoing retraining and monitoring."
      ],
      "metadata": {
        "id": "eK38LN8Vq6R8"
      }
    }
  ]
}