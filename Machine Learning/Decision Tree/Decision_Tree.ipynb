{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "9mWLHesqQ4Vu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81f67dbb"
      },
      "source": [
        "1. What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "->\n",
        "\n",
        "A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks.\n",
        "\n",
        "In classification, it works by recursively partitioning the data based on features to create a tree-like structure.\n",
        "\n",
        "Each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label.\n",
        "\n",
        "To classify a new instance, you traverse the tree from the root, following the branches based on the instance's feature values, until you reach a leaf node, which provides the predicted class.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "->\n",
        "\n",
        "Gini Impurity and Entropy are measures used to quantify the impurity or disorder of a set of samples.\n",
        "\n",
        "- Gini Impurity:\n",
        "\n",
        "Measures the probability of incorrectly classifying a randomly chosen element from the set if it were randomly labeled according to the distribution of labels in the subset.\n",
        "\n",
        "A Gini impurity of 0 means all elements belong to the same class.\n",
        "\n",
        "\n",
        "- Entropy:\n",
        "\n",
        "Measures the average amount of information needed to identify the class of an element in the set. A lower entropy indicates less impurity.\n",
        "\n",
        "These measures impact splits by guiding the algorithm to choose the feature and split point that results in the greatest reduction in impurity (or greatest information gain) at each node.\n",
        "\n",
        "The goal is to create nodes with samples that are as homogeneous as possible in terms of their class labels.\n",
        "\n",
        "\n",
        "\n",
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "->\n",
        "\n",
        "- Pre-Pruning:\n",
        "\n",
        "Stops the tree growth early during the training phase. It prevents the tree from growing beyond a certain depth or when the impurity reduction at a node is below a threshold.\n",
        "\n",
        "\n",
        "    - Practical Advantage:\n",
        "      \n",
        "      Faster training time and can prevent overfitting by limiting the complexity of the tree from the start.\n",
        "\n",
        "      \n",
        "- Post-Pruning:\n",
        "\n",
        "Grows the full tree first and then prunes back branches that provide little or no additional information. It typically uses a validation set to evaluate the performance of the pruned tree.\n",
        "\n",
        "    - Practical Advantage:\n",
        "    \n",
        "      Can sometimes lead to a more optimal tree structure by allowing the tree to explore all possible splits before removing the less useful ones.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "->\n",
        "\n",
        "Information Gain is the reduction in entropy or Gini impurity achieved by splitting a dataset based on a particular feature.\n",
        "\n",
        "It is calculated as the impurity of the parent node minus the weighted average of the impurities of the child nodes.\n",
        "\n",
        "Information Gain is important because it quantifies how much a feature helps in classifying the data.\n",
        "\n",
        "The Decision Tree algorithm chooses the feature with the highest information gain at each node to make the split, as this split is expected to best separate the data into different classes or values.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "->\n",
        "\n",
        "Common Real-World Applications:\n",
        "\n",
        "- Medical Diagnosis: Identifying potential diseases based on patient symptoms and medical history.\n",
        "\n",
        "- Credit Risk Assessment: Determining the likelihood of a loan applicant defaulting.\n",
        "\n",
        "- Customer Relationship Management (CRM): Predicting customer churn or identifying potential customers.\n",
        "\n",
        "- Fraud Detection: Identifying fraudulent transactions in financial data.\n",
        "\n",
        "- Image Classification: Categorizing images based on their features (though less common than other methods like deep learning for complex images).\n",
        "\n",
        "\n",
        "\n",
        "Advantages:\n",
        "\n",
        "- Easy to Understand and Interpret: The tree structure is intuitive and can be easily visualized.\n",
        "\n",
        "- Requires Little Data Preparation: Can handle both numerical and categorical data, and doesn't require feature scaling.\n",
        "\n",
        "- Can Handle Multi-Output Problems: Can predict multiple target variables simultaneously.\n",
        "\n",
        "- Non-Parametric: Makes no assumptions about the underlying data distribution.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "- Prone to Overfitting: Can create overly complex trees that perform well on training data but poorly on unseen data.\n",
        "\n",
        "- Instability: Small changes in the data can lead to significantly different tree structures.\n",
        "\n",
        "- Bias towards Features with More Levels: Features with a larger number of distinct values can be favored.\n",
        "\n",
        "- Difficulty with Complex Relationships: May not perform as well as other algorithms for capturing complex non-linear relationships in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Info:\n",
        "\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or provided CSV).\n",
        "\n",
        "● Boston Housing Dataset for regression tasks (sklearn.datasets.load_boston() or provided CSV)."
      ],
      "metadata": {
        "id": "pxfOTR_iRrkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "6. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "->\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier using the Gini criterion\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, dt_classifier.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgZ4MDA6RSIA",
        "outputId": "3d83d50c-1cfe-4a8a-b26c-0418f4f8c89a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "7. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare\n",
        "its accuracy to a fully-grown tree.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "->\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a fully-grown Decision Tree Classifier\n",
        "dt_full = DecisionTreeClassifier(random_state=42)\n",
        "dt_full.fit(X_train, y_train)\n",
        "y_pred_full = dt_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"Accuracy of fully-grown tree: {accuracy_full:.4f}\")\n",
        "\n",
        "# Train a Decision Tree Classifier with max_depth=3\n",
        "dt_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dt_pruned.fit(X_train, y_train)\n",
        "y_pred_pruned = dt_pruned.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "print(f\"Accuracy of tree with max_depth=3: {accuracy_pruned:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vynk19UCR3p0",
        "outputId": "3fc32ade-1add-4940-b1e2-4aa5ddcc05aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of fully-grown tree: 1.0000\n",
            "Accuracy of tree with max_depth=3: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "8. Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "->\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "# The Boston dataset is no longer included with scikit-learn 1.2.\n",
        "# We will fetch it from openml.\n",
        "boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
        "X, y = boston.data, boston.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = dt_regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(X.columns, dt_regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d_2ov25R4V0",
        "outputId": "0f34a3c1-5329-4a97-bfe7-1eb424966d3c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 10.4161\n",
            "Feature Importances:\n",
            "CRIM: 0.0513\n",
            "ZN: 0.0034\n",
            "INDUS: 0.0058\n",
            "CHAS: 0.0000\n",
            "NOX: 0.0271\n",
            "RM: 0.6003\n",
            "AGE: 0.0136\n",
            "DIS: 0.0707\n",
            "RAD: 0.0019\n",
            "TAX: 0.0125\n",
            "PTRATIO: 0.0110\n",
            "B: 0.0090\n",
            "LSTAT: 0.1933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "9. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "->\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [None, 2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(dt_classifier, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Get the best model\n",
        "best_dt_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the test set using the best model\n",
        "y_pred = best_dt_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the resulting model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Best Parameters: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMTFTrXdR48N",
        "outputId": "ba32a4c7-33dc-4ad5-f2d2-a07630e8fa64"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2}\n",
            "Model Accuracy with Best Parameters: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1746aa6"
      },
      "source": [
        "10. Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "\n",
        "And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "->\n",
        "\n",
        "Here is the step-by-step process:\n",
        "\n",
        "1. Data Loading and Initial Exploration:\n",
        "\n",
        "   - Load the dataset into a suitable data structure (e.g., pandas DataFrame).\n",
        "\n",
        "   - Perform initial exploration to understand the data structure, identify data types (numerical, categorical), and check for the presence of missing values.\n",
        "\n",
        "\n",
        "2. Handling Missing Values:\n",
        "\n",
        "   - Identification: Identify which features have missing values and the extent of missingness.\n",
        "\n",
        "   - Strategies: Choose appropriate strategies based on the nature and amount of missing data.\n",
        "   \n",
        "   - Common methods include:\n",
        "\n",
        "       i) Imputation:\n",
        "\n",
        "           - For numerical features: Impute with the mean, median, or mode.\n",
        "\n",
        "           - For categorical features: Impute with the mode or a constant value like 'Missing'.\n",
        "\n",
        "       ii) Deletion:\n",
        "       \n",
        "            - If a feature has a very high percentage of missing values or if rows with missing values are few, you might consider dropping the feature or the rows.\n",
        "\n",
        "       iii) Advanced Techniques:\n",
        "       \n",
        "            - More sophisticated methods like K-Nearest Neighbors (KNN) imputation or multiple imputation can also be considered, especially if the missingness is not random.\n",
        "\n",
        "\n",
        "3. Encoding Categorical Features:\n",
        "\n",
        "   - Identification: Identify all categorical features.\n",
        "\n",
        "   - Strategies: Convert categorical features into a numerical format that the Decision Tree model can understand.\n",
        "   \n",
        "   - Common methods include:\n",
        "\n",
        "       i) One-Hot Encoding:\n",
        "       \n",
        "       - Creates new binary columns for each category in the feature.\n",
        "       \n",
        "       - This is suitable for nominal categorical data where there is no intrinsic order.\n",
        "\n",
        "\n",
        "       ii) Label Encoding:\n",
        "       \n",
        "       - Assigns a unique integer to each category.\n",
        "       \n",
        "       - This is suitable for ordinal categorical data where there is a meaningful order.\n",
        "\n",
        "       \n",
        "       - Be cautious when using Label Encoding with Decision Trees if the order is not meaningful, as the model might interpret the assigned numbers as having an ordinal relationship.\n",
        "\n",
        "\n",
        "       - Other Encoding Schemes: Depending on the cardinality (number of unique categories), other methods like target encoding might be considered.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4. Splitting the Data:\n",
        "\n",
        "   - Split the dataset into training, validation (optional but recommended for hyperparameter tuning), and testing sets.\n",
        "   \n",
        "   - This ensures that the model is evaluated on unseen data.\n",
        "\n",
        "\n",
        "\n",
        "5. Training a Decision Tree Model:\n",
        "\n",
        "   - Import the Decision Tree Classifier from a library like scikit-learn (`sklearn.tree.DecisionTreeClassifier`).\n",
        "\n",
        "   - Instantiate the model, initially using default parameters or basic configurations (e.g., using the Gini criterion).\n",
        "\n",
        "   - Train the model on the training data (`fit()` method).\n",
        "\n",
        "\n",
        "\n",
        "6. Tuning its Hyperparameters:\n",
        "\n",
        "   - Identify Key Hyperparameters: Important hyperparameters for Decision Trees include `max_depth`, `min_samples_split`, `min_samples_leaf`, `criterion` (Gini or Entropy), etc.\n",
        "\n",
        "\n",
        "   - Hyperparameter Tuning Techniques:\n",
        "        - Use techniques like:\n",
        "            i) Grid Search:\n",
        "              \n",
        "              - Exhaustively searches over a specified range of hyperparameter values.\n",
        "\n",
        "            ii) Random Search:\n",
        "            \n",
        "              - Randomly samples hyperparameter values from a defined distribution.\n",
        "\n",
        "            iii) Cross-Validation:\n",
        "            \n",
        "              - Use cross-validation (e.g., k-fold cross-validation) during tuning to get a more robust estimate of model performance for each set of hyperparameters.\n",
        "\n",
        "\n",
        "   - Train the model with different hyperparameter combinations and evaluate its performance on the validation set.\n",
        "   \n",
        "   - Select the set of hyperparameters that yields the best performance on the validation set.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "7. Evaluating its Performance:\n",
        "\n",
        "   - Evaluate the final tuned model on the unseen test set.\n",
        "\n",
        "\n",
        "   - Evaluation Metrics: Choose appropriate evaluation metrics for classification tasks, such as:\n",
        "\n",
        "       i) Accuracy: The proportion of correctly classified instances.\n",
        "\n",
        "       ii) Precision: The proportion of true positive predictions among all positive predictions.\n",
        "\n",
        "       iii) Recall (Sensitivity): The proportion of true positive predictions among all actual positive instances.\n",
        "\n",
        "       iv) F1-Score: The harmonic mean of precision and recall, providing a balance between the two.\n",
        "\n",
        "       v) ROC AUC: The area under the Receiver Operating Characteristic curve, indicating the model's ability to distinguish between classes.\n",
        "\n",
        "       vi) Confusion Matrix: Provides a breakdown of true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "\n",
        "   - Analyze the evaluation results to understand the model's strengths and weaknesses.\n",
        "\n",
        "\n",
        "\n",
        "Business Value in the Real-World Setting:\n",
        "\n",
        "A Decision Tree model predicting whether a patient has a certain disease can provide significant business value to a healthcare company:\n",
        "\n",
        "   i) Early Diagnosis and Intervention:\n",
        "   \n",
        "          - The model can help identify patients at high risk of having the disease, allowing for earlier diagnosis and intervention, which can lead to better patient outcomes and potentially lower treatment costs in the long run.\n",
        "\n",
        "\n",
        "   ii) Resource Allocation:\n",
        "   \n",
        "          - By identifying high-risk patients, the healthcare company can allocate resources more effectively, prioritizing diagnostic tests, specialist consultations, and preventive measures for those who need them most.\n",
        "\n",
        "\n",
        "   iii) Personalized Treatment Plans:\n",
        "   \n",
        "        - The model's decision path can potentially reveal important factors contributing to the disease, which could inform the development of more personalized treatment plans for individual patients.\n",
        "\n",
        "\n",
        "   iv) Reducing Healthcare Costs:\n",
        "   \n",
        "        - Early identification and intervention can help prevent the progression of the disease to more severe stages, potentially reducing the need for expensive treatments and hospitalizations.\n",
        "\n",
        "\n",
        "   v) Improving Patient Care:\n",
        "   \n",
        "      - By providing healthcare professionals with a data-driven tool to assess disease risk, the model can support clinical decision-making and ultimately improve the quality of patient care.\n",
        "\n",
        "\n",
        "   vi) Research and Insights:\n",
        "   \n",
        "      - The feature importances from the Decision Tree can highlight which patient characteristics or medical history factors are most predictive of the disease, providing valuable insights for further medical research and understanding of the disease.\n",
        "\n",
        "\n",
        "   vii) Risk Stratification:\n",
        "   \n",
        "      - The model can help stratify patients into different risk categories (e.g., low, medium, high risk), enabling tailored approaches for follow-up and management."
      ]
    }
  ]
}