{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble Learning"
      ],
      "metadata": {
        "id": "9mWLHesqQ4Vu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81f67dbb"
      },
      "source": [
        "1. What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "->\n",
        "\n",
        "Ensemble learning is a machine learning technique where multiple models are trained to solve the same problem and combined to get better results.\n",
        "\n",
        "The key idea is that by combining the predictions of several models, the ensemble model is less likely to make mistakes than any single model.\n",
        "\n",
        "This is because different models will likely make different errors, and by averaging or voting on their predictions, these errors can be canceled out.\n",
        "\n",
        "\n",
        "2. What is the difference between Bagging and Boosting?\n",
        "\n",
        "->\n",
        "\n",
        "Bagging:\n",
        "\n",
        "  Involves training multiple models independently on different bootstrap samples (random subsets with replacement) of the training data.\n",
        "\n",
        "  The final prediction is typically the average (for regression) or majority vote (for classification) of the individual model predictions.\n",
        "\n",
        "  Bagging aims to reduce variance and is effective with unstable models like decision trees.\n",
        "\n",
        "  Random Forest is a popular example of a bagging method.\n",
        "\n",
        "\n",
        "Boosting:\n",
        "\n",
        "  Trains models sequentially, where each subsequent model focuses on correcting the errors made by the previous models.\n",
        "\n",
        "  It gives more weight to misclassified instances. The final prediction is a weighted sum of the individual model predictions.\n",
        "\n",
        "  Boosting aims to reduce bias and can convert weak learners into strong learners.\n",
        "\n",
        "  Gradient Boosting and AdaBoost are examples of boosting methods.\n",
        "\n",
        "\n",
        "3. What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "->\n",
        "\n",
        "Bootstrap sampling is a resampling technique where you create multiple datasets of the same size as the original dataset by randomly sampling with replacement. This means that some data points may appear multiple times in a single bootstrap sample, while others may not appear at all.\n",
        "\n",
        "In Bagging methods like Random Forest, bootstrap sampling plays a crucial role:\n",
        "\n",
        "  Creating Diverse Datasets: Each individual model (e.g., decision tree) in the ensemble is trained on a different bootstrap sample of the training data.\n",
        "\n",
        "  This introduces randomness and diversity among the individual models, as they are exposed to slightly different versions of the data.\n",
        "\n",
        "  Reducing Variance: By training models on these varied datasets and then combining their predictions (e.g., through averaging or voting), Bagging helps to reduce the variance of the overall ensemble model.\n",
        "\n",
        "  This is because the errors made by individual models on different data points tend to cancel each other out when their predictions are combined.\n",
        "\n",
        "\n",
        "\n",
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "->\n",
        "\n",
        "Out-of-Bag (OOB) samples are the data points that are not included in the bootstrap sample used to train a particular model in a Bagging ensemble.\n",
        "\n",
        "Since bootstrap sampling involves sampling with replacement, each bootstrap sample will typically leave out about one-third of the original training data. These left-out data points form the OOB sample for that specific model.\n",
        "\n",
        "The OOB score is a way to evaluate the performance of a Bagging ensemble without needing a separate validation set or resorting to cross-validation.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "  For each data point in the original training set, identify the individual models in the ensemble that did not use this data point in their training (i.e., the data point is in the OOB sample for these models).\n",
        "\n",
        "  Use these models to predict the outcome for that specific data point.\n",
        "  Combine these predictions (e.g., by averaging or voting) to get an OOB prediction for that data point.\n",
        "\n",
        "  Compare the OOB prediction with the actual target value for that data point.\n",
        "\n",
        "  The OOB score is then calculated by aggregating these comparisons across all data points in the training set. For classification, it's typically the accuracy; for regression, it might be the mean squared error.\n",
        "\n",
        "\n",
        "5. Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "->\n",
        "\n",
        "Single Decision Tree:\n",
        "\n",
        "  Calculation:\n",
        "\n",
        "      Feature importance in a single decision tree is typically calculated based on how much each feature reduces impurity (like Gini impurity or entropy) when it's used for splitting nodes. Features that result in larger reductions in impurity are considered more important.\n",
        "\n",
        "  Limitations:\n",
        "\n",
        "      Feature importance in a single tree can be highly volatile and sensitive to the specific training data and the tree's structure. Small changes in the data can lead to significant changes in which features are deemed important and their relative rankings. This makes the importance scores from a single tree less reliable and potentially misleading.\n",
        "\n",
        "\n",
        "Random Forest:\n",
        "\n",
        "  Calculation:\n",
        "      \n",
        "      Feature importance in a Random Forest is typically calculated by averaging the feature importance scores across all the individual decision trees in the ensemble. There are two common methods:\n",
        "  \n",
        "        Mean Decrease in Impurity (MDI):\n",
        "        \n",
        "            This is similar to the single tree approach, but the impurity reduction for each feature is averaged across all trees.\n",
        "        \n",
        "\n",
        "        Mean Decrease in Accuracy (MDA) or Permutation Importance:\n",
        "        \n",
        "            This method is generally preferred as it is less biased. It involves shuffling the values of a single feature in the OOB samples and measuring how much the model's performance (e.g., accuracy) decreases. A larger decrease indicates higher feature importance.\n",
        "        \n",
        "\n",
        "  Advantages:\n",
        "        \n",
        "      By averaging the importance scores across multiple trees trained on different bootstrap samples, the feature importance in a Random Forest is much more stable and robust than in a single tree.\n",
        "            \n",
        "      It provides a more reliable estimate of the true importance of each feature in predicting the target variable.\n",
        "            \n",
        "      It helps to mitigate the influence of noisy data or specific data splits that might artificially inflate or deflate the importance of a feature in a single tree."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "6. Write a Python program to:\n",
        "\n",
        "● Load the Breast Cancer dataset using\n",
        "\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "feature_names = breast_cancer.feature_names\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "# Using a fixed random_state for reproducibility\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Create a pandas Series for easier sorting and selection\n",
        "feature_importance_series = pd.Series(feature_importances, index=feature_names)\n",
        "\n",
        "# Get the top 5 most important features\n",
        "top_5_features = feature_importance_series.sort_values(ascending=False).head(5)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_5_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "cgZ4MDA6RSIA",
        "outputId": "8a4cfb95-f5bd-4454-d7d4-f8d1f8d28b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n6: Write a Python program to: \\n\\n● Load the Breast Cancer dataset using \\n\\nsklearn.datasets.load_breast_cancer() \\n\\n● Train a Random Forest Classifier \\n\\n● Print the top 5 most important features based on feature importance scores.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "7. Write a Python program to:\n",
        "\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "->\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree Classifier\n",
        "single_tree = DecisionTreeClassifier(random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "single_tree_pred = single_tree.predict(X_test)\n",
        "single_tree_accuracy = accuracy_score(y_test, single_tree_pred)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "# Using a fixed random_state for reproducibility\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42),\n",
        "                                n_estimators=10,\n",
        "                                random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_pred = bagging_clf.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy of single Decision Tree: {single_tree_accuracy:.4f}\")\n",
        "print(f\"Accuracy of Bagging Classifier: {bagging_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vynk19UCR3p0",
        "outputId": "a2e64d72-92c5-4668-8541-3be9ee109242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of single Decision Tree: 1.0000\n",
            "Accuracy of Bagging Classifier: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "8. Write a Python program to:\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "● Print the best parameters and final accuracy\n",
        "->\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset (using Breast Cancer dataset for classification example)\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'max_depth': [None, 10, 20, 30]\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Get the best estimator\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the final accuracy\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Final Accuracy with Best Parameters: {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d_2ov25R4V0",
        "outputId": "dca6d0a0-032d-4dec-86f2-f432e1b043b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 150}\n",
            "Final Accuracy with Best Parameters: 0.9708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "9. Write a Python program to:\n",
        "\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "->\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Bagging Regressor\n",
        "# Using a fixed random_state for reproducibility\n",
        "bagging_reg = BaggingRegressor(n_estimators=100, random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "bagging_pred = bagging_reg.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_pred)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "# Using a fixed random_state for reproducibility\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "# Print the Mean Squared Errors\n",
        "print(f\"Mean Squared Error of Bagging Regressor: {bagging_mse:.4f}\")\n",
        "print(f\"Mean Squared Error of Random Forest Regressor: {rf_mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMTFTrXdR48N",
        "outputId": "ba32a4c7-33dc-4ad5-f2d2-a07630e8fa64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2}\n",
            "Model Accuracy with Best Parameters: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1746aa6"
      },
      "source": [
        "10. You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to:\n",
        "\n",
        "● Choose between Bagging or Boosting\n",
        "\n",
        "● Handle overfitting\n",
        "\n",
        "● Select base models\n",
        "\n",
        "● Evaluate performance using cross-validation\n",
        "\n",
        "● Justify how ensemble learning improves decision-making in this real-world context.\n",
        "\n",
        "->\n",
        "\n",
        "Step-by-Step Approach:\n",
        "\n",
        "  Problem Understanding and Data Preparation:\n",
        "\n",
        "      Clearly define the target variable: Loan Default (binary classification: Yes/No).\n",
        "\n",
        "  Understand the available features:\n",
        "\n",
        "      Customer demographic data (age, income, etc.) and transaction history data (spending patterns, payment history, etc.).\n",
        "\n",
        "      Perform thorough data cleaning, handling missing values, outliers, and inconsistencies.\n",
        "\n",
        "  Feature Engineering:\n",
        "  \n",
        "      Create relevant features from transaction history (e.g., average transaction amount, frequency of transactions, late payment indicators) and combine them with demographic data.\n",
        "\n",
        "\n",
        "  Data Splitting:\n",
        "  \n",
        "      Split the data into training, validation (optional but recommended for tuning), and test sets. Ensure the split maintains the class distribution of the target variable (loan default is likely a rare event, so stratification is crucial).\n",
        "\n",
        "\n",
        "Choosing Between Bagging and Boosting:\n",
        "\n",
        "  Consider the nature of the problem:\n",
        "  \n",
        "      Loan default prediction is a critical task where both reducing variance (Bagging) and reducing bias (Boosting) are important.\n",
        "\n",
        "  Bagging (e.g., Random Forest):\n",
        "  \n",
        "      Often a good starting point. It's less prone to overfitting than Boosting and can handle noisy data well. Random Forests can provide good feature importance insights, which can be valuable for understanding drivers of default.\n",
        "\n",
        "  Boosting (e.g., Gradient Boosting, AdaBoost, XGBoost, LightGBM):\n",
        "  \n",
        "      Can often achieve higher accuracy by iteratively focusing on difficult cases. However, Boosting is more sensitive to hyperparameters and can overfit if not carefully tuned.\n",
        "\n",
        "  Decision:\n",
        "  \n",
        "      Start with both and compare performance. Random Forest is often a strong baseline. Boosting methods like XGBoost or LightGBM are powerful but require more careful tuning. Given the critical nature of loan default prediction, exploring both is recommended.\n",
        "\n",
        "\n",
        "Selecting Base Models:\n",
        "\n",
        "  For both Bagging and Boosting, decision trees are commonly used as base models (weak learners). They are interpretable and can capture non-linear relationships.\n",
        "\n",
        "  For Bagging (like Random Forest), the base models are typically deep, unpruned decision trees (which have high variance). Bagging then helps to reduce this variance.\n",
        "\n",
        "  For Boosting, the base models are usually shallow decision trees (stumps or trees with limited depth). Boosting then sequentially combines these weak learners to build a strong model.\n",
        "\n",
        "  Other models can be used as base learners, but decision trees are a standard and effective choice for many ensemble methods.\n",
        "\n",
        "\n",
        "Handling Overfitting:\n",
        "\n",
        "  Bagging:\n",
        "\n",
        "      Inherently helps reduce overfitting by averaging predictions from multiple models trained on different data subsets.\n",
        "      \n",
        "      However, individual trees in a Random Forest can still overfit.\n",
        "      \n",
        "      Techniques like limiting max_depth or setting min_samples_split in the base trees can further help.\n",
        "  \n",
        "  Boosting:\n",
        "\n",
        "      More prone to overfitting than Bagging. Strategies to mitigate overfitting include:\n",
        "\n",
        "        Regularization: Techniques like L1 or L2 regularization in boosting algorithms.\n",
        "\n",
        "        Shrinkage (Learning Rate): Using a small learning rate and a large number of estimators.\n",
        "\n",
        "        Subsampling: Using a fraction of the training data and/or features for each iteration.\n",
        "\n",
        "        Early Stopping: Monitoring performance on a validation set and stopping training when performance starts to degrade.\n",
        "\n",
        "\n",
        "  Hyperparameter Tuning:\n",
        "  \n",
        "      Crucial for both Bagging and Boosting, especially Boosting, to find the right balance between model complexity and generalization.\n",
        "\n",
        "  Cross-validation:\n",
        "  \n",
        "      As discussed below, it's essential for reliable performance estimation and hyperparameter tuning.\n",
        "\n",
        "\n",
        "Evaluating Performance using Cross-Validation:\n",
        "\n",
        "  Cross-validation is critical for getting a reliable estimate of how well the ensemble model will perform on unseen data.\n",
        "\n",
        "  k-Fold Cross-Validation:\n",
        "  \n",
        "      Divide the training data into k folds. Train the model k times, each time using a different fold as the validation set and the remaining k-1 folds for training.\n",
        "\n",
        "  Evaluation Metrics:\n",
        "  \n",
        "      For loan default prediction (a classification problem), appropriate metrics include:\n",
        "      Accuracy: Overall correct predictions (but can be misleading with imbalanced classes).\n",
        "\n",
        "      Precision: Of those predicted as default, how many actually defaulted. Important to minimize false positives (predicting default when the customer won't).\n",
        "\n",
        "      Recall (Sensitivity): Of those who actually defaulted, how many were correctly predicted. Important to minimize false negatives (failing to predict default when it occurs).\n",
        "      \n",
        "      F1-score: Harmonic mean of precision and recall, balancing both.\n",
        "      ROC AUC: Measures the model's ability to distinguish between the two classes.\n",
        "\n",
        "      Confusion Matrix: Provides a detailed breakdown of true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "\n",
        "  Use cross-validation to:\n",
        "\n",
        "      Compare different ensemble methods (Bagging vs. Boosting, different algorithms).\n",
        "\n",
        "      Tune hyperparameters (using GridSearchCV or RandomizedSearchCV within the cross-validation loop).\n",
        "\n",
        "      Get a more robust estimate of the model's performance metrics.\n",
        "\n",
        "\n",
        "Justifying How Ensemble Learning Improves Decision-Making in this Real-World Context:\n",
        "  Improved Accuracy and Robustness:\n",
        "  \n",
        "      Ensemble models generally achieve higher accuracy and are more robust to noise and outliers than single models. This is crucial in financial decision-making where prediction errors can have significant consequences.\n",
        "\n",
        "  Reduced Risk:\n",
        "  \n",
        "      By combining multiple perspectives, the ensemble model is less likely to be influenced by the biases or limitations of a single model. This leads to more reliable predictions of loan default.\n",
        "  \n",
        "  Better Handling of Complex Relationships:\n",
        "  \n",
        "      Ensemble methods can capture complex non-linear relationships in the data that might be missed by simpler models.\n",
        "  \n",
        "  Feature Importance Insights:\n",
        "  \n",
        "      Ensemble models like Random Forest and Gradient Boosting can provide insights into which features are most predictive of loan default.\n",
        "      \n",
        "      This information can be used by the financial institution to understand the key risk factors and potentially adjust lending policies or offer targeted financial advice.\n",
        "  \n",
        "  Confidence in Decisions:\n",
        "  \n",
        "      A more accurate and robust model provides greater confidence in automated decision-making processes for loan approvals or risk assessments."
      ]
    }
  ]
}