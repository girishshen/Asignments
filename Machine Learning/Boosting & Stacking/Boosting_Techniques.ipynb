{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting Techniques"
      ],
      "metadata": {
        "id": "9mWLHesqQ4Vu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaaf85d9"
      },
      "source": [
        "```\n",
        "1. What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "\n",
        "->\n",
        "\n",
        "Boosting is an ensemble learning technique in machine learning that combines the predictions of multiple weak learners (typically simple models like decision trees with limited depth) to create a strong learner.\n",
        "\n",
        "How it improves weak learners:\n",
        "\n",
        "\n",
        "i) Sequential Learning:\n",
        "\n",
        "Unlike bagging (like Random Forests) where models are trained independently, boosting trains models sequentially. Each new model is built to correct the errors made by the previous ones.\n",
        "\n",
        "\n",
        "ii) Focus on Difficult Examples:\n",
        "\n",
        "In each iteration, boosting gives more weight to the instances that were misclassified by the previous models. This forces the new models to focus on the \"hard-to-learn\" examples.\n",
        "\n",
        "\n",
        "iii) Weighted Voting (or Combination):\n",
        "\n",
        "The final prediction is a weighted combination of the predictions from all the weak learners. Models that perform better on the training data are often given more weight.\n",
        "\n",
        "\n",
        "2. What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "\n",
        "->\n",
        "\n",
        "The main difference lies in how they weigh the instances and the type of errors they try to correct.\n",
        "\n",
        "AdaBoost focuses on misclassified instances by adjusting their weights, while Gradient Boosting focuses on the residuals (the errors) from the previous model by fitting the next model to these residuals.\n",
        "\n",
        "\n",
        "3. How does regularization help in XGBoost?\n",
        "\n",
        "->\n",
        "\n",
        "Regularization in XGBoost helps to prevent overfitting. It adds penalty terms to the objective function that the model tries to minimize.\n",
        "\n",
        "These penalties discourage complex models and help the model generalize better to unseen data. XGBoost supports L1 (Lasso) and L2 (Ridge) regularization.\n",
        "\n",
        "\n",
        "4. Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "->\n",
        "\n",
        "CatBoost is considered efficient for handling categorical data primarily due to its innovative approach called \"Ordered Boosting\" and its native handling of categorical features.\n",
        "\n",
        "It uses a permutation-driven approach to calculate gradients, which helps prevent overfitting and bias when dealing with categorical features.\n",
        "\n",
        "Additionally, it employs a special method for encoding categorical features on the fly during training, avoiding the need for extensive preprocessing like one-hot encoding, which can be computationally expensive and lead to high-dimensional sparse data.\n",
        "\n",
        "\n",
        "5. What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "\n",
        "->\n",
        "\n",
        "Boosting techniques are often preferred over bagging methods in real-world applications where high accuracy and the ability to handle complex relationships are crucial. Some examples include:\n",
        "\n",
        "i) Fraud Detection:\n",
        "\n",
        "Boosting models can effectively identify complex patterns in transaction data to detect fraudulent activities.\n",
        "\n",
        "ii) Customer Churn Prediction:\n",
        "\n",
        "Boosting can be used to predict which customers are likely to leave, allowing companies to intervene and retain them.\n",
        "\n",
        "iii) Medical Diagnosis:\n",
        "\n",
        "Boosting can help in building accurate diagnostic models by combining the insights from various medical features.\n",
        "\n",
        "iv) Search Ranking:\n",
        "\n",
        "Boosting algorithms like Gradient Boosted Decision Trees (often used in libraries like XGBoost, LightGBM, and CatBoost) are widely used in search engines to rank results based on relevance.\n",
        "\n",
        "v) Image and Speech Recognition:\n",
        "\n",
        "While deep learning is dominant in these areas, boosting can still be used in certain scenarios or as part of a larger ensemble."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasets:\n",
        "\n",
        "● Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "\n",
        "● Use sklearn.datasets.fetch_california_housing() for regression tasks."
      ],
      "metadata": {
        "id": "ZOsRZDP6EPU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "6. Write a Python program to:\n",
        "\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Print the model accuracy\n",
        "\n",
        "->\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an AdaBoost Classifier\n",
        "adaboost = AdaBoostClassifier(random_state=42)\n",
        "adaboost.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred = adaboost.predict(X_test)\n",
        "accuracy = round((accuracy_score(y_test, y_pred)) * 100, 2)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"AdaBoost Classifier Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgtCOK-bDmH4",
        "outputId": "0f4de179-fb52-4758-c6e6-f5b363d0c6b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 96.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "7. Write a Python program to:\n",
        "\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "\n",
        "● Evaluate performance using R-squared score\n",
        "\n",
        "->\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the dataset\n",
        "california_housing = fetch_california_housing()\n",
        "X, y = california_housing.data, california_housing.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred = gbr.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the R-squared score\n",
        "print(f\"Gradient Boosting Regressor R-squared: {r2}\")"
      ],
      "metadata": {
        "id": "SgG72KPmEdNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "8. Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Tune the learning rate using GridSearchCV\n",
        "\n",
        "● Print the best parameters and accuracy\n",
        "\n",
        "->\n",
        "'''\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgb_clf = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "best_xgb = grid_search.best_estimator_\n",
        "y_pred = best_xgb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Best XGBoost Classifier Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Af1hsZbEnbE",
        "outputId": "5d734b40-88df-426b-ef07-2b7bd9da0580"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'learning_rate': 0.2}\n",
            "Best XGBoost Classifier Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "9. Write a Python program to:\n",
        "\n",
        "● Train a CatBoost Classifier\n",
        "\n",
        "● Plot the confusion matrix using seaborn\n",
        "\n",
        "->\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the CatBoost Classifier\n",
        "# CatBoost handles categorical features automatically if specified,\n",
        "# but for the Breast Cancer dataset, all features are numerical.\n",
        "# We can still use CatBoost as a general classifier.\n",
        "catboost = CatBoostClassifier(iterations=100, # Number of boosting iterations\n",
        "                              learning_rate=0.1,\n",
        "                              depth=6,\n",
        "                              loss_function='Logloss',\n",
        "                              eval_metric='Accuracy',\n",
        "                              random_state=42,\n",
        "                              verbose=0) # Set verbose to 0 to reduce output during training\n",
        "\n",
        "catboost.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = catboost.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# You can also print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "E64Y5TVBoywt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You're working for ographics and transaction behavior.\n",
        "\n",
        "The dataset using customer demissing values, and has both numeric and categorical feat is imbalanced, contains ma FinTech company trying to predict loan defaultures.\n",
        "\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "● Hyperparameter tuning strategy\n",
        "\n",
        "● Evaluation metrics you'd choose and why\n",
        "\n",
        "● How the business would benefit from your model  \n",
        "\n",
        "->\n",
        "\n",
        "i) Data Preprocessing & Handling Missing/Categorical Values:\n",
        "\n",
        "  Address missing values using imputation or removal. Handle categorical features with one-hot encoding, target encoding, or CatBoost's native methods. Consider feature scaling and techniques like oversampling/undersampling for class imbalance.\n",
        "\n",
        "\n",
        "ii) Choice between AdaBoost, XGBoost, or CatBoost:\n",
        "\n",
        "  CatBoost is a strong candidate due to its native handling of categorical features and missing values, which aligns well with the dataset description. XGBoost is also viable with proper preprocessing. AdaBoost might be less effective on complex, imbalanced data. The final choice might involve experimentation.\n",
        "\n",
        "\n",
        "iii) Hyperparameter Tuning Strategy:\n",
        "  Use techniques like GridSearchCV, RandomizedSearchCV, or Bayesian Optimization with cross-validation to find optimal hyperparameters (e.g., learning rate, number of estimators, tree depth, regularization).\n",
        "\n",
        "\n",
        "iv) Evaluation Metrics you'd choose and why:\n",
        "\n",
        "For imbalanced data, use metrics beyond accuracy, such as Precision, Recall, F1-score, ROC AUC, and PR AUC. These metrics provide a more comprehensive view of the model's performance, especially on the minority class (defaults). The specific choice depends on the business's tolerance for false positives vs. false negatives.\n",
        "\n",
        "\n",
        "v) How the business would benefit from your model:\n",
        "\n",
        "Benefits include reduced financial losses from defaults, improved risk assessment for better lending decisions, optimized resource allocation for collections, enhanced customer segmentation, and a competitive advantage through better risk management."
      ],
      "metadata": {
        "id": "H831COdVoqib"
      }
    }
  ]
}